{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35262182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(r\"C:\\Users\\Rudra\\Downloads\\Output Data Structure.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19437be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL   \n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...  \\\n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE SCORE NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE   \n",
       "0             NaN            NaN             NaN                 NaN  \\\n",
       "1             NaN            NaN             NaN                 NaN   \n",
       "2             NaN            NaN             NaN                 NaN   \n",
       "3             NaN            NaN             NaN                 NaN   \n",
       "4             NaN                            NaN                 NaN   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX   \n",
       "0                  NaN                          NaN        NaN  \\\n",
       "1                  NaN                          NaN        NaN   \n",
       "2                  NaN                          NaN        NaN   \n",
       "3                  NaN                          NaN        NaN   \n",
       "4                  NaN                          NaN        NaN   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT   \n",
       "0                               NaN                 NaN         NaN  \\\n",
       "1                               NaN                 NaN         NaN   \n",
       "2                               NaN                 NaN         NaN   \n",
       "3                               NaN                 NaN         NaN   \n",
       "4                               NaN                 NaN         NaN   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                NaN                NaN              NaN  \n",
       "1                NaN                NaN              NaN  \n",
       "2                NaN                NaN              NaN  \n",
       "3                NaN                NaN              NaN  \n",
       "4                NaN                NaN              NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c86d6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\Rudra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rudra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rudra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title_tag = soup.find('title')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else 'No Title'\n",
    "            article_tag = soup.find('article') or soup.find('div', class_='content')\n",
    "            article_text = article_tag.get_text(strip=True) if article_tag else 'No Article Text Found'\n",
    "            return f\"{title}\\n\\n{article_text}\"\n",
    "        else:\n",
    "            return \"Failed to retrieve the article\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "def analyze_text(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    \n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "    \n",
    "    \n",
    "    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "    \n",
    "   \n",
    "    complex_words = [word for word in words if sum(1 for char in word if char in 'aeiou') > 2]\n",
    "    percentage_complex_words = len(complex_words) / len(words)\n",
    "    \n",
    "   \n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    \n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "    \n",
    "    \n",
    "    complex_word_count = len(complex_words)\n",
    "    \n",
    "    \n",
    "    word_count = len(words)\n",
    "    \n",
    "    \n",
    "    syllable_per_word = sum(sum(1 for char in word if char in 'aeiou') for word in words) / len(words)\n",
    "    \n",
    "    \n",
    "    personal_pronouns = sum(1 for word in words if word in ['i', 'we', 'my', 'ours', 'us'])\n",
    "    \n",
    "    \n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    return {\n",
    "        'positive_score': positive_score,\n",
    "        'negative_score': negative_score,\n",
    "        'polarity_score': polarity_score,\n",
    "        'subjectivity_score': subjectivity_score,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'percentage_complex_words': percentage_complex_words,\n",
    "        'fog_index': fog_index,\n",
    "        'avg_words_per_sentence': avg_words_per_sentence,\n",
    "        'complex_word_count': complex_word_count,\n",
    "        'word_count': word_count,\n",
    "        'syllable_per_word': syllable_per_word,\n",
    "        'personal_pronouns': personal_pronouns,\n",
    "        'avg_word_length': avg_word_length\n",
    "    }\n",
    "\n",
    "\n",
    "output_dir = \"Desktop/articles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    article_text = extract_article_text(url)\n",
    "    \n",
    "    if \"Failed to retrieve the article\" not in article_text and \"No Article Text Found\" not in article_text:\n",
    "        analysis = analyze_text(article_text)\n",
    "        analysis['URL_ID'] = url_id\n",
    "        results.append(analysis)\n",
    "        \n",
    "        output_file_path = os.path.join(output_dir, f\"{url_id}.txt\")\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(article_text)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(os.path.join(output_dir, 'text_analysis_results.xlsx'), index=False)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3206b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
